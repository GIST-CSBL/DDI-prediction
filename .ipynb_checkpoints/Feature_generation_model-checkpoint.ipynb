{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.regularizers import *\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, Callback\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import metrics\n",
    "\n",
    "\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "# session = tf.Session(config=config)\n",
    "# K.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(df):\n",
    "    x_fp = df.filter(regex='fp')\n",
    "    y_label = df.drop(x_fp, axis=1)\n",
    "    \n",
    "    return x_fp, y_label\n",
    "\n",
    "def split_train_test(x):\n",
    "    x = x.sample(frac=1)\n",
    "    \n",
    "    train = round(x.shape[0]*0.8)\n",
    "    print(train)\n",
    "    \n",
    "    return x.iloc[0:train,:], x.iloc[train:,:]\n",
    "\n",
    "def split_dataset_descriptor(df):\n",
    "    x_fp = df.filter(regex='fp')\n",
    "    temp = df.drop(x_fp, axis=1)\n",
    "    x_desc = temp.iloc[:,0:100]\n",
    "    y_label = temp.iloc[:,100:]\n",
    "\n",
    "    return x_desc, y_label\n",
    "\n",
    "def split_dataset_descriptor_both(df):\n",
    "    x_fp = df.filter(regex='fp')\n",
    "    temp = df.drop(x_fp, axis=1)\n",
    "    x_desc = temp.iloc[:,0:100]\n",
    "    y_label = temp.iloc[:,100:]\n",
    "\n",
    "    return x_fp, x_desc, y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "#    Pearson's correlation loss\n",
    "###################################\n",
    "def tf_pearson(y_true, y_pred):\n",
    "    return tf.contrib.metrics.streaming_pearson_correlation(y_pred, y_true)[1]\n",
    "\n",
    "\n",
    "###################################\n",
    "#    Cosine annealing scheduler\n",
    "#    https://github.com/4uiiurz1/keras-cosine-annealing\n",
    "###################################\n",
    "class CosineAnnealingScheduler(Callback):\n",
    "    def __init__(self, T_max, eta_max, eta_min=0, verbose=0):\n",
    "        super(CosineAnnealingScheduler, self).__init__()\n",
    "        self.T_max = T_max\n",
    "        self.eta_max = eta_max\n",
    "        self.eta_min = eta_min\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if not hasattr(self.model.optimizer, 'lr'):\n",
    "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
    "        lr = self.eta_min + (self.eta_max - self.eta_min) * (1 + math.cos(math.pi * epoch / self.T_max)) / 2\n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "        if self.verbose > 0:\n",
    "            print('\\nEpoch %05d: CosineAnnealingScheduler setting learning '\n",
    "                  'rate to %s.' % (epoch + 1, lr))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        logs['lr'] = K.get_value(self.model.optimizer.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "#    Model architectures\n",
    "###################################\n",
    "\n",
    "def struct_only_model(input_dim):\n",
    "    K.clear_session()\n",
    "    \n",
    "    input_drug = Input(shape=(input_dim,))\n",
    "\n",
    "    hidden = Dense(512, input_dim=input_dim, activation='relu', kernel_regularizer=l2(0.01))(input_drug)\n",
    "    hidden = BatchNormalization()(hidden)\n",
    "    hidden = Dense(128, input_dim=input_dim, activation='relu', kernel_regularizer=l2(0.01))(hidden)\n",
    "    hidden = BatchNormalization()(hidden)\n",
    "    output = Dense(978, activation='tanh', kernel_regularizer=l2(0.01))(hidden)\n",
    "    output = Lambda(lambda x:10*x)(output)\n",
    "    \n",
    "    \n",
    "    feature = Model(inputs=input_drug, outputs=output)\n",
    "    feature.compile(loss=\"mean_squared_error\", optimizer=Adam(0.0001), metrics=[metrics.mse, tf_pearson])\n",
    "    \n",
    "    return feature\n",
    "\n",
    "def descriptor_only_model(input_dim):\n",
    "    K.clear_session()\n",
    "    \n",
    "    input_drug = Input(shape=(input_dim,))\n",
    "\n",
    "    hidden = Dense(128, input_dim=input_dim, activation='relu', kernel_regularizer=l2(0.01))(input_drug)\n",
    "    hidden = BatchNormalization()(hidden)\n",
    "    output = Dense(978, activation='tanh', kernel_regularizer=l2(0.01))(hidden)\n",
    "    output = Lambda(lambda x:10*x)(output)\n",
    "    \n",
    "    \n",
    "    feature = Model(inputs=input_drug, outputs=output)\n",
    "    feature.compile(loss=\"mean_squared_error\", optimizer=Adam(0.0001), metrics=[metrics.mse, tf_pearson])\n",
    "    \n",
    "    return feature\n",
    "\n",
    "def structure_descriptor_model(structure_input, prop_input):\n",
    "    K.clear_session()\n",
    "    \n",
    "    structure = Input(shape=(structure_input,))\n",
    "\n",
    "    hidden = Dense(512, input_dim=structure_input, activation='relu', kernel_regularizer=l2(0.01))(structure)\n",
    "    hidden = BatchNormalization()(hidden)\n",
    "    hidden = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(hidden)\n",
    "    hidden = BatchNormalization()(hidden)\n",
    "    \n",
    "    prop = Input(shape=(prop_input,))\n",
    "    \n",
    "    hidden2 = Dense(128, input_dim=prop_input, activation='relu', kernel_regularizer=l2(0.01))(prop)\n",
    "    hidden2 = BatchNormalization()(hidden2)\n",
    "    \n",
    "    concat = concatenate([hidden, hidden2])\n",
    "    output = Dense(978, activation='tanh', kernel_regularizer=l2(0.01))(concat)\n",
    "    \n",
    "    output = Lambda(lambda x:10*x)(output)\n",
    "    \n",
    "\n",
    "    feature = Model(inputs=[structure, prop], outputs=output)\n",
    "    feature.compile(loss=\"mean_squared_error\", optimizer=Adam(0.0001), metrics=[metrics.mse, tf_pearson])\n",
    "    \n",
    "    return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "#    Model train\n",
    "###################################\n",
    "\n",
    "# Type in directory to save model\n",
    "save_path = ''\n",
    "\n",
    "def train_cv(model_type, input_data, epochs):\n",
    "    early_stopping = EarlyStopping(monitor='val_mean_squared_error', patience=30)\n",
    "\n",
    "    x_fp, x_desc, y_train = split_dataset_descriptor_both(input_data)\n",
    "    \n",
    "    \n",
    "    # 1. structure only model\n",
    "    # 2. property only model\n",
    "    # 3. Structure + property model\n",
    "    if (model_type == 1):\n",
    "        print('Model with compound fingerprints')\n",
    "        checkpoint = ModelCheckpoint(save_path+'cv_test_structOnly.h5', monitor='loss', verbose=1, save_best_only=True, mode='min', period=1)\n",
    "        model = struct_only_model(x_fp.shape[1])\n",
    "        _x = x_fp\n",
    "\n",
    "    elif (model_type == 2):\n",
    "        print('Model with compound properties')\n",
    "        checkpoint = ModelCheckpoint(save_path+'cv_test_propOnly.h5', monitor='loss', verbose=1, save_best_only=True, mode='min', period=1)\n",
    "        model = descriptor_only_model(x_fp.shape[1])\n",
    "        _x = x_desc\n",
    "\n",
    "    elif (model_type == 3):\n",
    "        print('Model with both compound fingerprints and properties')\n",
    "        checkpoint = ModelCheckpoint(save_path+'cv_test_scaled.h5', monitor='loss', verbose=1, save_best_only=True, mode='min', period=1)\n",
    "        model = structure_descriptor_model(x_fp.shape[1], x_desc.shape[1])\n",
    "        _x = [x_fp, x_desc]\n",
    "\n",
    "    callbacks = [CosineAnnealingScheduler(T_max=20, eta_max=1e-4), checkpoint]\n",
    "\n",
    "\n",
    "    # ============================================================================\n",
    "    K.get_session().run(tf.local_variables_initializer())\n",
    "\n",
    "    \n",
    "    _y = y_train\n",
    "    model.fit(_x,_y, validation_split=0.1, shuffle=True, verbose=0, batch_size=64, epochs=epochs, callbacks=callbacks)\n",
    "\n",
    "    pd.DataFrame(model.history.history).to_csv(save_path+'model_cv_performance.csv')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(data, model, model_type):    \n",
    "    x_fp_total, x_desc_total, y_total = split_dataset_descriptor_both(data)\n",
    "    \n",
    "    # 1. structure only model\n",
    "    # 2. property only model\n",
    "    # 3. Structure + property model\n",
    "    if (model_type == 1):\n",
    "        recon_x = model.predict([x_fp_total])\n",
    "    elif (model_type == 2):\n",
    "        recon_x = model.predict([x_desc_total])\n",
    "    elif (model_type == 3):\n",
    "        recon_x = model.predict([x_fp_total, x_desc_total])\n",
    "            \n",
    "    corr_list = list()\n",
    "    for i in range(recon_x.shape[0]):\n",
    "        corr_list.append(np.corrcoef(y_total.values[i], recon_x[i])[0][1])\n",
    "    print('Average Correlation: ', np.mean(corr_list))\n",
    "\n",
    "    return corr_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load input data\n",
    "data_path = ''\n",
    "input_data = pd.read_csv(data_path+'example_feature_model_input.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(save_path, model_name):\n",
    "    custom_objects = {'tf_pearson':tf_pearson}\n",
    "\n",
    "    test_model = keras.models.load_model(save_path+model_name+'.h5', custom_objects=custom_objects)\n",
    "\n",
    "    print(model_name + ' loaded === ')\n",
    "    return test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_cv(model_type=3, input_data=input_data, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_list = model_predict(input_data, model, model_type=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
